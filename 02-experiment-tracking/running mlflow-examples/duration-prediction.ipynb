{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/CVT--MLOps/running mlflow-examples/mlruns/1', creation_time=1725645892822, experiment_id='1', last_update_time=1725645892822, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from pyarrow) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow  # or fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (2024.5.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from fastparquet) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from fastparquet) (2.0.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from fastparquet) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from fastparquet) (2024.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/mlflow-env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_dataframe(filename):\n",
    "    # Use pd.read_parquet to read a Parquet file\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    # Convert datetime columns to pandas datetime objects if they exist\n",
    "    if 'lpep_dropoff_datetime' in df.columns:\n",
    "        df['lpep_dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "    if 'lpep_pickup_datetime' in df.columns:\n",
    "        df['lpep_pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet')\n",
    "df_val = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76518, 64572)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PULocationID and DOLocationID to strings before concatenating\n",
    "df_train['PU_DO'] = df_train['PULocationID'].astype(str) + '_' + df_train['DOLocationID'].astype(str)\n",
    "df_val['PU_DO'] = df_val['PULocationID'].astype(str) + '_' + df_val['DOLocationID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PULocationID  DOLocationID    PU_DO\n",
      "0            43           151   43_151\n",
      "1           166           239  166_239\n",
      "2            41            42    41_42\n",
      "3           168            75   168_75\n",
      "4           265           265  265_265\n",
      "   PULocationID  DOLocationID    PU_DO\n",
      "0           130           205  130_205\n",
      "1           152           244  152_244\n",
      "2           152            48   152_48\n",
      "3           152           241  152_241\n",
      "4            75            42    75_42\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the modified DataFrames\n",
    "print(df_train[['PULocationID', 'DOLocationID', 'PU_DO']].head())\n",
    "print(df_val[['PULocationID', 'DOLocationID', 'PU_DO']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['duration'] = (df_train['lpep_dropoff_datetime'] - df_train['lpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df_val['duration'] = (df_val['lpep_dropoff_datetime'] - df_val['lpep_pickup_datetime']).dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mlflow-env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(53.60353017829977)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the 'models' directory if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mlflow-env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"preethi\")\n",
    "\n",
    "    mlflow.log_param(\"train-data-path\", \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\")\n",
    "    mlflow.log_param(\"valid-data-path\", \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-02.parquet\")\n",
    "\n",
    "    alpha = 0.1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "    lr = Lasso(alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/lin_reg.bin\", artifact_path=\"models_pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "valid = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:28<00:00,  1.74trial/s, best loss: 58.220117119463886]\n",
      "Best Hyperparameters: {'learning_rate': np.float64(0.05050139888691769), 'max_depth': np.float64(52.0), 'min_child_weight': np.float64(19.707747714803663), 'reg_alpha': np.float64(0.007384211677294675), 'reg_lambda': np.float64(0.3332024698599833)}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Define the search space with the corrected objective parameter\n",
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0), #exponential(-3), exp(0), - [0.05, 1]\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:squarederror',  # Updated objective\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Define the objective function (example)\n",
    "def objective(params):\n",
    "    # Your model training and evaluation code here\n",
    "    # Example:\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.datasets import load_diabetes\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    X, y = load_diabetes(return_X_y=True)\n",
    "    model = XGBRegressor(**params)\n",
    "    # Use cross-validation to evaluate the model\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse = (-scores.mean()) ** 0.5\n",
    "    return rmse\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'models/preprocessor.b' not found. Please generate the file before running the code.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "\n",
    "# Automatically log model parameters and metrics\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Ensure that the directory and file exist\n",
    "os.makedirs(\"models\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Replace this with the actual code to generate the preprocessor.b file if necessary\n",
    "# For now, let's assume it should already exist. Check for its existence.\n",
    "if not os.path.exists(\"models/preprocessor.b\"):\n",
    "    print(\"File 'models/preprocessor.b' not found. Please generate the file before running the code.\")\n",
    "else:\n",
    "    # Loop through the models\n",
    "    for model_class in (RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, LinearSVR):\n",
    "\n",
    "        with mlflow.start_run():  # Start a new MLflow run\n",
    "\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"train-data-path\", \"./data/green_tripdata_2021-01.csv\")\n",
    "            mlflow.log_param(\"valid-data-path\", \"./data/green_tripdata_2021-02.csv\")\n",
    "\n",
    "            # Log the artifact (only if it exists)\n",
    "            mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "            # Initialize and train the model\n",
    "            mlmodel = model_class()\n",
    "            mlmodel.fit(X_train, y_train)\n",
    "\n",
    "            # Predict and calculate RMSE\n",
    "            y_pred = mlmodel.predict(X_val)\n",
    "            rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "            # Log RMSE metric\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
